{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Untitled0.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyMNVGLWMDV9piBJk76xdCei"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","source":["!nvidia-smi"],"metadata":{"id":"TVIP7q-3OW05","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1652509631975,"user_tz":-540,"elapsed":307,"user":{"displayName":"정준모","userId":"02195039556837670251"}},"outputId":"a31e33ea-cfe7-419d-a4ce-c3aab48ca990"},"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["NVIDIA-SMI has failed because it couldn't communicate with the NVIDIA driver. Make sure that the latest NVIDIA driver is installed and running.\n","\n"]}]},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"id":"nF-THEBAOW3I","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1652509652555,"user_tz":-540,"elapsed":18115,"user":{"displayName":"정준모","userId":"02195039556837670251"}},"outputId":"27adf236-ac42-427e-c460-5d2a0bafe667"},"execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"code","source":["from cProfile import label\n","import os"],"metadata":{"id":"S_XijwUpOW6G","executionInfo":{"status":"ok","timestamp":1652509658664,"user_tz":-540,"elapsed":279,"user":{"displayName":"정준모","userId":"02195039556837670251"}}},"execution_count":4,"outputs":[]},{"cell_type":"code","source":["import torch\n","import torch.nn as nn\n","from torch.utils.data import Dataset, DataLoader, random_split\n","from torch.nn import functional as F\n","import torchvision.transforms as transforms\n","import pytorch_lightning as pl\n","import numpy as np\n","import pandas as pd\n","from PIL import Image\n","import itertools"],"metadata":{"id":"BEJQbg76OW8u","colab":{"base_uri":"https://localhost:8080/","height":391},"executionInfo":{"status":"error","timestamp":1652509670672,"user_tz":-540,"elapsed":3840,"user":{"displayName":"정준모","userId":"02195039556837670251"}},"outputId":"7dc81db7-35ce-4da0-c3b6-bc98219baca0"},"execution_count":5,"outputs":[{"output_type":"error","ename":"ModuleNotFoundError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)","\u001b[0;32m<ipython-input-5-846e98de5178>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mfunctional\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorchvision\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransforms\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtransforms\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mpytorch_lightning\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mpl\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'pytorch_lightning'","","\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"],"errorDetails":{"actions":[{"action":"open_url","actionText":"Open Examples","url":"/notebooks/snippets/importing_libraries.ipynb"}]}}]},{"cell_type":"code","source":["class ImgDataset(Dataset):\n","    def __init__(self, df, images_folder, transform = None):\n","        self.df = df\n","        self.images_folder = images_folder\n","        self.transform = transform\n","        self.class2index = {'bag':0, 'bed':1, 'chair':2, 'coffeetable':3, 'cup':4, 'kitchentools':5, 'lamp':6, 'laptop':7, 'LivingSofa':8, 'pot':9, 'shoe':10}\n","\n","    def __len__(self):\n","        return len(self.df)\n","\n","    def __getitem__(self, index):\n","        ####################################################\n","        # 입력 받은 csv를 통해 데이터셋을 구현하고\n","        # 이비지 형태를 변환하는 코드 구현\n","\n","        ####################################################\n","        return image, label\n","\n","\n","class CNN(pl.LightningModule):\n","    def __init__(self, batch_size, image_size):\n","        super(CNN, self).__init__()\n","\n","        self.transform = transforms.Compose([\n","        ####################################################\n","        # 이미지 형태를 tensor로 변환하기 위한 부분\n","        # torchvision.transforms 참고\n","\n","        ####################################################\n","        ])\n","        self.batch_size = batch_size\n","\n","        ####################################################\n","        # 모델에 사용될 layer들을 구현\n","\n","        ####################################################\n","\n","    def forward(self, x):\n","        batch_size = x.size(0)\n","\n","        ####################################################\n","        # 모델의 layer를 활용해 모델 구조를 구현\n","\n","        ####################################################\n","        return output\n","\n","    def prepare_data(self):\n","        train_df = pd.read_csv('train_data.csv')\n","        train_dataset = ImgDataset(train_df, 'train', self.transform)\n","\n","        # split dataset to train, val, test ! ==> 0.8 / 0.2\n","        train_dataset, val_dataset = random_split(train_dataset, [int(len(train_dataset) * 0.8), (len(train_dataset) - int(len(train_dataset) * 0.8))])\n","        \n","        valid_df = pd.read_csv('val_data.csv')\n","        test_dataset = ImgDataset(valid_df, 'val', self.transform)\n","\n","        self.train_dataset = train_dataset\n","        self.val_dataset = val_dataset\n","        self.test_dataset = test_dataset\n","\n","    def train_dataloader(self):\n","        return DataLoader(self.train_dataset, batch_size=self.batch_size, shuffle=True)\n","\n","    def val_dataloader(self):\n","        return DataLoader(self.val_dataset, batch_size=self.batch_size)\n","\n","    def test_dataloader(self):\n","        return DataLoader(self.test_dataset, batch_size=self.batch_size)\n","\n","    def configure_optimizers(self):\n","        param_optimizer = list(self.named_parameters())\n","        optimizer_grouped_parameters = [{\"params\": [p for n, p in param_optimizer]}]\n","        optimizer = torch.optim.Adam(optimizer_grouped_parameters, lr=3e-5)\n","        return optimizer\n","\n","    def training_step(self, batch, batch_idx):\n","        data, target = batch\n","\n","        output = self(data)\n","        loss = F.cross_entropy(output, target)\n","\n","        return loss\n","\n","    def validation_step(self, batch, batch_idx):\n","        data, target = batch\n","\n","        output = self(data)\n","        loss = F.cross_entropy(output, target)\n","        pred = output.argmax(dim=1, keepdim=True)\n","        correct = pred.eq(target.view_as(pred)).sum().item()\n","        return {\"val_loss\": loss, \"correct\": correct}\n","\n","    def validation_epoch_end(self, outputs):\n","        avg_loss = torch.stack([x['val_loss'] for x in outputs]).mean()\n","        tensorboard_logs = {'val_loss': avg_loss}\n","        self.log(\"val_loss\", avg_loss, prog_bar=True)\n","        return {'avg_val_loss': avg_loss, 'log': tensorboard_logs}\n","\n","    def test_step(self, batch, batch_idx):\n","        data, target = batch\n","\n","        output = self(data)\n","        pred = output.argmax(dim=1, keepdim=True)\n","        correct = pred.eq(target.view_as(pred)).sum().item()\n","        return {\"correct\": correct}\n","\n","    def test_epoch_end(self, outputs):\n","        all_correct = sum([output[\"correct\"] for output in outputs])\n","        accuracy = all_correct / len(self.test_dataloader().dataset)\n","        self.log(\"Accuracy\", accuracy)\n","        return accuracy\n","\n","    def predict_step(self, batch, batch_idx, dataloader_idx=0):\n","        data, target = batch\n","        output = self(data)\n","        y_hat = output.argmax(dim=1, keepdim=True)\n","        return y_hat\n","\n","\n","if __name__ == \"__main__\":\n","    pl.seed_everything(42)  # set seed\n","    torch.manual_seed(42)\n","\n","    cnn_model = CNN(batch_size=32, image_size=256)\n","\n","    cnn_trainer = pl.Trainer(gpus=1, max_epochs=20)\n","    cnn_trainer.fit(cnn_model)\n","\n","    cnn_trainer.test()\n","\n","    ####################################################\n","    # 학습된 모델을 저장하는 코드 구현\n","    \n","    ####################################################\n","\n","    ####################################################\n","    # 저장된 모델을 불러오고\n","    # prediction 데이터셋을 생성해 불러온 모델에 입력하여\n","    # prediction 결과를 출력하는 코드 구현\n","    \n","    ####################################################\n","\n","    ####################################################\n","    # prediction 결과를 csv파일로 저장하는 코드 구현\n","    # Pandas\n","\n","    ####################################################\n","\n","\n"],"metadata":{"id":"smLX0QzlOW_X"},"execution_count":null,"outputs":[]}]}